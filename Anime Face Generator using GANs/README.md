# Anime Face Generator | DCGAN | Deep Convolutional Neural Network

Here is a video demonstration:

[![YouTube](http://i.ytimg.com/vi/muw9ZHTmwOA/hqdefault.jpg)](https://www.youtube.com/watch?v=muw9ZHTmwOA)

This project uses Deep Convolutional Generative Adversarial Networks (DCGANs) to generate anime faces. DCGANs are a type of Generative Adversarial Network (GAN) that have been particularly successful in generating high-quality images.

## Table of Contents

- [Introduction](#introduction)
- [Project Structure](#project-structure)
- [Dataset](#dataset)
- [Installation](#installation)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [DCGAN](#dcgan)
- [Deployment](#deployment)
- [References](#references)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Introduction

This project utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) to generate realistic anime faces. By leveraging the power of GANs, specifically DCGANs, this project aims to create high-quality, novel anime character faces from scratch. The GAN framework consists of two neural networks, the Generator and the Discriminator, that are trained together to produce increasingly convincing images. This repository includes the complete code for training the models, generating images, and evaluating the results

## Project Structure

```
.
├── dataset
│   ├── train
│   ├── readme.txt
├── models
│   ├── ModelDCGAN.py
├── jupyter notebooks
│   ├── AnimeFaceGeneratorDCGAN.ipynb
├── utils
│   ├── utils.py
│   ├── model.py
│   ├── config.py
├── pytorch saved models
│   ├── AnimeFaceGeneratorDCGAN.pth
├── Generated Images
│   ├── images generated during training -> 1 to 50
├── README.md
├── DCGAN PAPER.pdf
├── output_video.mp4
└── requirements.txt
```


## Dataset

Sure! Here's a section about the dataset for your README:

---

## Dataset

The dataset used for training the DCGAN model consists of thousands of anime face images. These images have been curated to provide a diverse set of examples, encompassing various styles and features typical of anime characters. The dataset is preprocessed to ensure consistency in size and quality, which helps in stabilizing the training process and improving the quality of the generated images. You can find the dataset in the `dataset/` directory within this repository. 

---

## Installation

Clone the repository and install the required dependencies:

```bash
git clone https://github.com/saketjha34/Python-Deep-Learning-Projects.git
cd Python-Deep-Learning-Projects/Anime%20Face%20Generator%20using%20GANs
pip install -r requirements.txt
```

## Usage

To train the model, run:

```bash
python models/ModelDCGAN.py  
```

To view model performances and benchmarks check out:

```bash
python Jupyter Notebooks/AnimeFaceGeneratorDCGAN.ipynb
```

For more detailed instructions, refer to the `utils/utils.py` , `utils/config.py` and `utils/model.py` directory, which contains all the necessary scripts for data preprocessing, training, and evaluation.

## Model Architecture

The model is built from scratch using convolutional neural networks like ResNet18 and AlexNet. The architecture details are available in the `utils/models.py` directory.

### AlexNet

Generator
The generator in a DCGAN takes random noise as input and transforms it through a series of transposed convolutional layers, batch normalization, and ReLU activations to produce a synthetic image. This network aims to create realistic-looking anime faces that can deceive the discriminator.

Discriminator
The discriminator is a convolutional neural network that classifies images as real or fake. It consists of convolutional layers, batch normalization, and Leaky ReLU activations. Its goal is to distinguish between real anime faces and those generated by the generator, thereby improving the generator's output through adversarial training.

For more details, refer to the original paper: [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).

### DCGAN

#### Overview
The paper *"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"* by Alec Radford, Luke Metz, and Soumith Chintala introduced the DCGAN architecture. This architecture aims to improve the stability of GAN training and to leverage deep convolutional networks for both the generator and discriminator models.

#### Generator Model
The generator network uses a series of transposed convolutional layers to transform a vector of random noise into a synthetic image. Key components include:
- **Transposed Convolutions**: Used to upsample the input, converting low-resolution feature maps to high-resolution images.
- **Batch Normalization**: Applied after each transposed convolution to stabilize training by normalizing layer inputs.
- **ReLU Activations**: Employed in all layers except the output layer, where a Tanh activation is used to ensure the output is in the range \([-1, 1]\).

#### Discriminator Model
The discriminator is a convolutional neural network designed to classify images as real or fake. Key components include:
- **Convolutional Layers**: Extract features from input images through a series of convolutions.
- **Batch Normalization**: Applied after each convolution to stabilize training and improve convergence.
- **Leaky ReLU Activations**: Used in all layers to allow a small gradient when the unit is not active, mitigating the vanishing gradient problem.

#### Training Guidelines
The training process for DCGANs involves alternately updating the generator and discriminator:
1. **Discriminator Training**: Update the discriminator by maximizing the probability of assigning the correct label to both real images from the dataset and fake images from the generator.
2. **Generator Training**: Update the generator by minimizing the probability of the discriminator correctly classifying its outputs as fake. This involves backpropagating the discriminator's error through the generator's weights.

Key training practices include:
- **Using Batch Normalization**: Stabilizes training by normalizing the inputs to each layer.
- **Leaky ReLU in Discriminator**: Prevents dying ReLU problem by allowing a small gradient flow when the neuron is not active.
- **Tanh Activation in Generator Output**: Ensures the output image pixels are in the range \([-1, 1]\), matching the preprocessing of input images.

For more details, refer to the original paper: [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).


## Deployment

The trained image generator is saved as `.pth` file in the `pytorch saved models` directory. These files can be used for further deployment purposes. You can load the models in PyTorch using the following code:

```python
import torch
import torch.nn as nn
from  utils.config import IMAGE_SIZE, IMG_CHANNELS, NUM_WORKERS, BATCH_SIZE , NOISE_DIM, LEARNING_RATE, EPOCHS

class Generator(nn.Module):
    def __init__(self , noise_channels , img_channels):
        super(Generator , self).__init__()

        self.Network = nn.Sequential(

            self._create_block(in_channels=noise_channels, out_channels=512, kernel_size=4, padding=0 , stride=1),
            self._create_block(in_channels=512, out_channels=256, kernel_size=4, padding=1 , stride=2),   
            self._create_block(in_channels=256, out_channels=128, kernel_size=4, padding=1 , stride=2),   
            self._create_block(in_channels=128, out_channels=64, kernel_size=4, padding=1 , stride=2), 

            nn.ConvTranspose2d(in_channels=64, out_channels=img_channels, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )


    def _create_block(self,in_channels, out_channels , kernel_size , padding ,stride):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride, padding=padding,bias= False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(),
        )

    def forward(self, x):
        return self.Network(x)

# Load Generator model
generator = Generator( noise_channels = NOISE_DIM , img_channels = IMG_CHANNELS )
generator.load_state_dict(torch.load('pytorch saved models/AnimeFaceDCGANs.pth'))
```
## References

- Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks." arXiv preprint arXiv:1511.06434 (2015). [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)
- Kaggle Dataset [link](https://www.kaggle.com/datasets/splcher/animefacedataset)

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! If you have any improvements or new models to add, please follow these steps:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature-branch`).
3. Commit your changes (`git commit -am 'Add new feature'`).
4. Push to the branch (`git push origin feature-branch`).
5. Create a new Pull Request.

## License

This project is licensed under the MIT License. See the [LICENSE](../LICENSE) file for more details.

## Contact

For any questions or suggestions, please open an issue or contact me @ saketjha0324@gmail.com. or [Linkedin](https://www.linkedin.com/in/saketjha34/)

---

Happy coding!
