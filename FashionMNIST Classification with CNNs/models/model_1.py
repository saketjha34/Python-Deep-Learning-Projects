# -*- coding: utf-8 -*-
"""FashionMNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CNXWkwtFnU8DA__rp0bSVFvTWznMtX-V

# 03. PyTorch IMAGE CLASSIFICATION

[Computer vision](https://en.wikipedia.org/wiki/Computer_vision) is the art of teaching a computer to see.

For example, it could involve building a model to classify whether a photo is of a cat or a dog ([binary classification](https://developers.google.com/machine-learning/glossary#binary-classification)).

Or whether a photo is of a cat, dog or chicken ([multi-class classification](https://developers.google.com/machine-learning/glossary#multi-class-classification)).

Or identifying where a car appears in a video frame ([object detection](https://en.wikipedia.org/wiki/Object_detection)).

Or figuring out where different objects in an image can be separated ([panoptic segmentation](https://arxiv.org/abs/1801.00868)).

![example computer vision problems](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-computer-vision-problems.png)
*Example computer vision problems for binary classification, multiclass classification, object detection and segmentation.*

## What we're going to cover

We're going to apply the PyTorch Workflow we've been learning in the past couple of sections to computer vision.

![a PyTorch workflow with a computer vision focus](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-pytorch-computer-vision-workflow.png)

Specifically, we're going to cover:

| **Topic** | **Contents** |
| ----- | ----- |
| **0. Computer vision libraries in PyTorch** | PyTorch has a bunch of built-in helpful computer vision libraries, let's check them out.  |
| **1. Load data** | To practice computer vision, we'll start with some images of different pieces of clothing from [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). |
| **2. Prepare data** | We've got some images, let's load them in with a [PyTorch `DataLoader`](https://pytorch.org/docs/stable/data.html) so we can use them with our training loop. |
| **3. Model 0: Building a baseline model** | Here we'll create a multi-class classification model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop**. |
| **4. Making predictions and evaluting model 0** | Let's make some predictions with our baseline model and evaluate them. |
| **5. Setup device agnostic code for future models** | It's best practice to write device-agnostic code, so let's set it up. |
| **6. Model 1: Adding non-linearity** | Experimenting is a large part of machine learning, let's try and improve upon our baseline model by adding non-linear layers. |
| **7. Model 2: Convolutional Neural Network (CNN)** | Time to get computer vision specific and introduce the powerful convolutional neural network architecture. |
| **8. Comparing our models** | We've built three different models, let's compare them. |
| **9. Evaluating our best model** | Let's make some predictons on random images and evaluate our best model. |
| **10. Making a confusion matrix** | A confusion matrix is a great way to evaluate a classification model, let's see how we can make one. |
| **11. Saving and loading the best performing model** | Since we might want to use our model for later, let's save it and make sure it loads back in correctly. |

## 0. Computer vision libraries in PyTorch

Before we get started writing code, let's talk about some PyTorch computer vision libraries you should be aware of.

| PyTorch module | What does it do? |
| ----- | ----- |
| [`torchvision`](https://pytorch.org/vision/stable/index.html) | Contains datasets, model architectures and image transformations often used for computer vision problems. |
| [`torchvision.datasets`](https://pytorch.org/vision/stable/datasets.html) | Here you'll find many example computer vision datasets for a range of problems from image classification, object detection, image captioning, video classification and more. It also contains [a series of base classes for making custom datasets](https://pytorch.org/vision/stable/datasets.html#base-classes-for-custom-datasets). |
| [`torchvision.models`](https://pytorch.org/vision/stable/models.html) | This module contains well-performing and commonly used computer vision model architectures implemented in PyTorch, you can use these with your own problems. |
| [`torchvision.transforms`](https://pytorch.org/vision/stable/transforms.html) | Often images need to be transformed (turned into numbers/processed/augmented) before being used with a model, common image transformations are found here. |
| [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) | Base dataset class for PyTorch.  |
| [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#module-torch.utils.data) | Creates a Python iterable over a dataset (created with `torch.utils.data.Dataset`). |

> **Note:** The `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes aren't only for computer vision in PyTorch, they are capable of dealing with many different types of data.

Now we've covered some of the most important PyTorch computer vision libraries, let's import the relevant dependencies.
"""

import torch
from torch import nn
import torchvision
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

"""## 1. Getting a dataset

To begin working on a computer vision problem, let's get a computer vision dataset.

We're going to start with FashionMNIST.

MNIST stands for Modified National Institute of Standards and Technology.

The [original MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) contains thousands of examples of handwritten digits (from 0 to 9) and was used to build computer vision models to identify numbers for postal services.

[FashionMNIST](https://github.com/zalandoresearch/fashion-mnist), made by Zalando Research, is a similar setup.

Except it contains grayscale images of 10 different kinds of clothing.

![example image of FashionMNIST](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-fashion-mnist-slide.png)
*`torchvision.datasets` contains a lot of example datasets you can use to practice writing computer vision code on. FashionMNIST is one of those datasets. And since it has 10 different image classes (different types of clothing), it's a multi-class classification problem.*

Later, we'll be building a computer vision neural network to identify the different styles of clothing in these images.

PyTorch has a bunch of common computer vision datasets stored in `torchvision.datasets`.

Including FashionMNIST in [`torchvision.datasets.FashionMNIST()`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html).

To download it, we provide the following parameters:
* `root: str` - which folder do you want to download the data to?
* `train: Bool` - do you want the training or test split?
* `download: Bool` - should the data be downloaded?
* `transform: torchvision.transforms` - what transformations would you like to do on the data?
* `target_transform` - you can transform the targets (labels) if you like too.

Many other datasets in `torchvision` have these parameter options.
"""

train_data = datasets.FashionMNIST(
    train = True,
    root = 'dataset/',
    download = True,
    transform = ToTensor(),
    target_transform=None
)

test_data = datasets.FashionMNIST(
    root="dataset/",
    train=False, # get test data
    download=True,
    transform=ToTensor()
)

image, label = train_data[0]
image, label

image.shape

len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)

class_names = train_data.classes
class_names

"""### 1.2 Visualizing our data"""

image , target = train_data[69]
plt.imshow(image.squeeze())
plt.title(f'{class_names[target]} : {target}' )
plt.show()

rows, cols = 5,5
def plot_images(rows,cols):
    torch.manual_seed(42)
    fig = plt.figure(figsize=(12, 12))
    for i in range(1,rows*cols+1):
        random_idx = torch.randint(0 , len(train_data), size = [1]).item()
        image, target = train_data[random_idx]
        fig.add_subplot(rows,cols,i)
        plt.title(f'{class_names[target]} : {target}' )
        plt.axis(False);
        plt.imshow(image.squeeze(), cmap='gray')
plot_images(rows=rows,cols = cols)

"""## 2. Prepare DataLoader

Now we've got a dataset ready to go.

The next step is to prepare it with a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) or `DataLoader` for short.

The `DataLoader` does what you think it might do.

It helps load data into a model.

For training and for inference.

It turns a large `Dataset` into a Python iterable of smaller chunks.

These smaller chunks are called **batches** or **mini-batches** and can be set by the `batch_size` parameter.

Why do this?

Because it's more computationally efficient.

In an ideal world you could do the forward pass and backward pass across all of your data at once.

But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches.

It also gives your model more opportunities to improve.

With **mini-batches** (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch).

What's a good batch size?

[32 is a good place to start](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) for a fair amount of problems.

But since this is a value you can set (a **hyperparameter**) you can try all different kinds of values, though generally powers of 2 are used most often (e.g. 32, 64, 128, 256, 512).

![an example of what a batched dataset looks like](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/03-batching-fashionmnist.png)
*Batching FashionMNIST with a batch size of 32 and shuffle turned on. A similar batching process will occur for other datasets but will differ depending on the batch size.*

Let's create `DataLoader`'s for our training and test sets.
"""

from torch.utils.data import DataLoader

BATCH_SIZE = 32

train_dl = DataLoader(
    train_data , shuffle = True , batch_size=BATCH_SIZE
)

test_dl = DataLoader(
    test_data , batch_size = BATCH_SIZE , shuffle = False
)

len(train_dl) , len(test_dl)

train_features_batch, train_targets_batch = next(iter(train_dl))
train_features_batch.shape, train_targets_batch.shape

train_features_batch[0]

"""## 3. Model 0: Build a baseline model

Data loaded and prepared!

Time to build a **baseline model** by subclassing `nn.Module`.

A **baseline model** is one of the simplest models you can imagine.

You use the baseline as a starting point and try to improve upon it with subsequent, more complicated models.

Our baseline will consist of two [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers.

We've done this in a previous section but there's going to one slight difference.

Because we're working with image data, we're going to use a different layer to start things off.

And that's the [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer.

`nn.Flatten()` compresses the dimensions of a tensor into a single vector.

This is easier to understand when you see it.
"""

from torch import nn
class FashionMNIST(nn.Module):
    def __init__(self , input_shape:int , hidden_layer : int , output_shape:int):
        super().__init__()
        self.layer_stack = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_features=input_shape , out_features=hidden_layer),
            nn.Linear(in_features=hidden_layer, out_features=output_shape)
        )

    def forward(self, x):
        return self.layer_stack(x)

INPUT_SHAPE = 784
OUTPUT_SHAPE = len(class_names)
HIDDEN_LAYERS = 16

torch.manual_seed(69)
model_0 = FashionMNIST(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE,hidden_layer=HIDDEN_LAYERS)
model_0.to(device)

"""### 3.1 Setup loss, optimizer and evaluation metrics


> **Note:** Rather than importing and using our own accuracy function or evaluation metric(s), you could import various evaluation metrics from the [TorchMetrics package](https://torchmetrics.readthedocs.io/en/latest/).
"""

# !pip install torchmetrics

from torchmetrics import Accuracy
accuracy_score = Accuracy(task="multiclass", num_classes=len(class_names)).to(device)

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.01)

"""### 3.3 Creating a training loop and training a model on batches of data

Beautiful!

Looks like we've got all of the pieces of the puzzle ready to go, a timer, a loss function, an optimizer, a model and most importantly, some data.

Let's now create a training loop and a testing loop to train and evaluate our model.

We'll be using the same steps as the previous notebook(s), though since our data is now in batch form, we'll add another loop to loop through our data batches.

Our data batches are contained within our `DataLoader`s, `train_dataloader` and `test_dataloader` for the training and test data splits respectively.

A batch is `BATCH_SIZE` samples of `X` (features) and `y` (labels), since we're using `BATCH_SIZE=32`, our batches have 32 samples of images and targets.

And since we're computing on batches of data, our loss and evaluation metrics will be calculated **per batch** rather than across the whole dataset.

This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader.

Let's step through it:
1. Loop through epochs.
2. Loop through training batches, perform training steps, calculate the train loss *per batch*.
3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.
4. Print out what's happening.
5. Time it all (for fun).

A fair few steps but...

...if in doubt, code it out.
"""

# Import tqdm for progress bar
from tqdm.auto import tqdm

torch.manual_seed(69)

EPOCHS = 15

for epoch in tqdm(range(EPOCHS)):
    print(f"Epoch: {epoch}\n-------")
    ### Training
    train_loss, train_accuracy = 0, 0

    for batch , (X_train,train_targets) in enumerate(train_dl):
        X_train, train_targets = X_train.to(device), train_targets.to(device)
        model_0.train()

        train_preds = model_0(X_train)

        loss = loss_fn(train_preds , train_targets)
        train_loss +=loss
        train_acc = accuracy_score(train_targets,train_preds.argmax(dim=1))
        train_accuracy+=train_acc

         # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()
        # print(f' Batch : {batch} | train_loss : {loss} | train_accuracy = {train_acc}')

    train_loss /= len(train_dl)
    train_accuracy /= len(train_dl)


    ## testing
    test_loss, test_accuracy = 0, 0
    model_0.eval()
    with torch.inference_mode():
        for X_test , test_targets in test_dl:
            X_test, test_targets = X_test.to(device), test_targets.to(device)
            test_preds = model_0(X_test)

            test_loss += loss_fn(test_preds,test_targets)

            test_acc = accuracy_score(test_preds.argmax(dim=1), test_targets)

        test_loss /= len(test_dl)

        # Divide total accuracy by length of test dataloader (per batch)
        test_accuracy /= len(test_dl)

    print(f"\nTrain loss: {train_loss:.5f} | Train Accuracy: {train_accuracy:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.5f}%\n")

"""## 4. Make predictions and get Model 0 results

Since we're going to be building a few models, it's a good idea to write some code to evaluate them all in similar ways.

Namely, let's create a function that takes in a trained model, a `DataLoader`, a loss function and an accuracy function.

The function will use the model to make predictions on the data in the `DataLoader` and then we can evaluate those predictions using the loss function and accuracy function.
"""

with torch.inference_mode():
        for(X_test, test_targets) in test_dl:
            X_test, test_targets = X_test.to(device), test_targets.to(device)
            test_preds = model_0(X_test)
            # print(test_preds)

            # test_loss += loss_fn(test_preds, test_taregts)
            test_accuracy += accuracy_score(test_targets,test_preds.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)

        # Scale loss and acc to find the average loss/acc per batch
        # test_loss /= len(test_dl)
        test_accuracy /= len(test_dl)
print(test_accuracy)

def eval_model(model:torch.nn.Module,
                train_dl : torch.utils.data.DataLoader,
                test_dl : torch.utils.data.DataLoader,
                loss_fn :torch.nn.Module ,
                accuracy_fn):

    ## Training
    train_loss, train_accuracy = 0, 0
    for X_train,train_targets in train_dl:
        X_train, train_targets = X_train.to(device), train_targets.to(device)
        train_preds = model(X_train)

        loss = loss_fn(train_preds , train_targets)
        train_loss +=loss
        train_acc = accuracy_fn(train_targets,train_preds.argmax(dim=1))
        train_accuracy+=train_acc

    train_loss /= len(train_dl)
    train_accuracy /= len(train_dl)

    ## Testing
    test_loss, test_accuracy = 0, 0
    model.eval()
    with torch.inference_mode():
        for(X_test, test_targets) in test_dl:
            X_test, test_targets = X_test.to(device), test_targets.to(device)
            test_preds = model(X_test)

            test_accuracy += accuracy_fn(test_targets,test_preds.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)

        # Scale loss and acc to find the average loss/acc per batch

        test_accuracy /= len(test_dl)
    return {"model_name": model, # only works when model was created with a class
            "train_loss": train_loss.item(),
            "train_accuracy": train_accuracy.item(),
            "test_accuracy": test_accuracy.item()}

eval_model(model=model_0, train_dl=train_dl,test_dl=test_dl,loss_fn=loss_fn,accuracy_fn=accuracy_score)

"""### 5 Functionizing training and test loops

So far we've been writing train and test loops over and over.

Let's write them again but this time we'll put them in functions so they can be called again and again.

And because we're using device-agnostic code now, we'll be sure to call `.to(device)` on our feature (`X`) and target (`y`) tensors.

For the training loop we'll create a function called `train_step()` which takes in a model, a `DataLoader` a loss function and an optimizer.

The testing loop will be similar but it'll be called `test_step()` and it'll take in a model, a `DataLoader`, a loss function and an evaluation function.

> **Note:** Since these are functions, you can customize them in any way you like. What we're making here can be considered barebones training and testing functions for our specific classification use case.
"""

def train_step(model : torch.nn.Module,
               train_dl : torch.utils.data.DataLoader,
               loss_fn : torch.nn.Module,
               optimizer : torch.optim.Optimizer,
               accuracy_fn ,
               device : torch.device = device):

    ### Training
    train_loss, train_accuracy = 0, 0
    model.to(device)
    for batch , (X_train,train_targets) in enumerate(train_dl):
        model.train()
        X_train, train_targets = X_train.to(device), train_targets.to(device)

        train_preds = model(X_train)

        loss = loss_fn(train_preds , train_targets)
        train_loss +=loss
        train_acc = accuracy_fn(train_targets,train_preds.argmax(dim=1))
        train_accuracy+=train_acc

         # 3. Optimizer zero grad
        optimizer.zero_grad()

        # 4. Loss backward
        loss.backward()

        # 5. Optimizer step
        optimizer.step()
        # print(f' Batch : {batch} | train_loss : {loss} | train_accuracy = {train_acc}')

    train_loss /= len(train_dl)
    train_accuracy /= len(train_dl)

    print(f"Train loss: {train_loss:.5f} | Train accuracy: {train_accuracy:.5f}%")

def test_step(model : torch.nn.Module,
               test_dl : torch.utils.data.DataLoader,
               loss_fn : torch.nn.Module,
               accuracy_fn ,
               device : torch.device = device):

    test_loss, test_accuracy = 0, 0
    model.to(device)
    model.eval()

    with torch.inference_mode():
        for X_test , test_taregts in test_dl:
            X_test, train_targets = X_test.to(device), train_targets.to(device)
            test_preds = model_0(X_test)

            test_loss += loss_fn(test_preds,test_targets)

            test_accuracy += accuracy_fn(test_preds.argmax(dim=1), test_targets)

        test_loss /= len(test_dl)

        # Divide total accuracy by length of test dataloader (per batch)
        test_accuracy /= len(test_dl)
        print(f"Test loss: {test_loss:.5f} | Test accuracy: {test_accuracy:.2f}%\n")

